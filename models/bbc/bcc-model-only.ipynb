{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Lab: Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"padding-right:10px;\" src=\"figures_wk7/topic_modeling.png\" width=400><br>\n",
    "\n",
    "This week's assignment will focus on text analysis of BBC News articles.\n",
    "\n",
    "## Our Dataset: \n",
    "**Dataset:** bbc.csv(Provided in folder assign_wk7)<br>\n",
    "Consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005. <br>\n",
    "Class Labels: 5 (business, entertainment, politics, sport, tech)\n",
    "\n",
    "## Text Analytics Lab\n",
    "\n",
    "**Objective:** \n",
    "To demostrate all of the text analysis techniques covered int his week's lecture material. Your submission needs to include the following:\n",
    "   - Preparation of the text data for analysis\n",
    "       * Elimination of stopwords, punctuation, digits, lowercase\n",
    "   - Identify the 10 most frequently used words in the text\n",
    "       * How about the ten least frequently used words? \n",
    "       * How does lemmatization change the most/least frequent words?\n",
    "           - Explain and demonstrate this topic\n",
    "   - Generate a world cloud for the text\n",
    "   - Demonstrate the generation of n-grams and part of speech tagging\n",
    "   - Create a Topic model of the text\n",
    "       * Find the optimal number of topics\n",
    "       * test the accuracy of your model\n",
    "       * Display your results 2 different ways.\n",
    "           1) Print the topics and explain any insights at this point.\n",
    "           2) Graph the topics and explain any insights at this point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables:\n",
    "\n",
    "Upload your notebook's .ipynb file and your topic_model_viz.html page this week.\n",
    "   \n",
    "**Important:** Make sure your provide complete and thorough explanations for all of your analysis. You need to defend your thought processes and reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "> Graphic comes from https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lester/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lester/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lester/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open data file and explore columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type                                               news\n",
       "0  business   Quarterly profits at US media giant TimeWarne...\n",
       "1  business   The dollar has hit its highest level against ...\n",
       "2  business   The owners of embattled Russian oil giant Yuk...\n",
       "3  business   British Airways has blamed high fuel prices f...\n",
       "4  business   Shares in UK drinks and food firm Allied Dome..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from the csv file\n",
    "df = pd.read_csv('bbc.csv', sep='\\t')\n",
    "df.head()\n",
    "# drop coumn filename and title\n",
    "df = df.drop(['filename', 'title'], axis=1)\n",
    "# rename the column text to content\n",
    "df = df.rename(columns={'content': 'news'})\n",
    "df = df.rename(columns={'category': 'type'})\n",
    "df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the text data for analysis\n",
    "The text data is preprocessed by tokenization and removal of unwanted tokens. NLTK's English stopword list was utilized, along with custom exceptions, to filter out common words that don't carry significant meaning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok = df.copy()\n",
    "df_tok['news'] = df_tok['news'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US', 'UK', 'TV', 'EU', 'PC', 'BT', 'MP', 'ID', 'GM', 'FA']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_letters_tokens = df_tok['news'].apply(lambda x: [i for i in x if len(i) == 2 and i.upper() == i and i.isalpha()])\n",
    "# get the list of acronyms\n",
    "acronyms = two_letters_tokens.explode().value_counts().index.tolist()[:10]\n",
    "acronyms\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted tokens: punctuation, numbers, and stopwords\n",
    "- Added some stopwords ranking too high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "# remove stop words identified\n",
    "stop_words.append('also')\n",
    "stop_words.append('first')\n",
    "stop_words.append('last')\n",
    "\n",
    "# convert to lower case only if the word is not an acronym\n",
    "df_tok['news'] = df_tok['news'].apply(lambda x: [item.lower() if item not in acronyms else item for item in x])\n",
    "\n",
    "# remove stop words\n",
    "df_tok['news'] = df_tok['news'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# remove punctuation\n",
    "df_tok['news'] = df_tok['news'].apply(lambda x: [item for item in x if item.isalpha()])\n",
    "\n",
    "# remove words with length less than 3 and if the word is not an acronym\n",
    "df_tok['news'] = df_tok['news'].apply(lambda x: [item for item in x if len(item) > 2 or item in acronyms])\n",
    "\n",
    "# remove digits\n",
    "df_tok['news'] = df_tok['news'].apply(lambda x: [item for item in x if not item.isdigit()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_lemmatized = df_tok.copy()\n",
    "df_lemmatized['news'] = df_lemmatized['news'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency after lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling and Category naming:\n",
    "The Latent Dirichlet Allocation (LDA) algorithm was applied to identify optimal topic categories based on the Coherence metric. This technique helped to uncover latent patterns and themes within the text data.\n",
    "\n",
    "The OpenAI's GPT-3 model (Davinci) was used to generate category names based on the keywords extracted from each topic. These category names provided further insights into the underlying topics and improved interpretability of the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<24022 unique tokens: ['US', 'account', 'adjust', 'advert', 'advertising']...>\n",
      "Dictionary<7737 unique tokens: ['US', 'account', 'adjust', 'advert', 'advertising']...>\n",
      "2225\n"
     ]
    }
   ],
   "source": [
    "# create dictionary and corpus\n",
    "id2word = Dictionary(df_lemmatized['news'])\n",
    "print(id2word)\n",
    "# remove extremes\n",
    "id2word.filter_extremes(no_below=5, no_above=0.5)\n",
    "print(id2word)\n",
    "corpus = [id2word.doc2bow(text) for text in df_lemmatized['news']]\n",
    "print(len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gensim lda models with 5-11 topics\n",
    "lda_models = [gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=i,\n",
    "                                                random_state=42,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                per_word_topics=True) for i in range(6, 7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence score for each model\n",
    "coherence_scores = []\n",
    "for i, lda in enumerate(lda_models):\n",
    "    coherence_model = CoherenceModel(model=lda, texts=df_lemmatized['news'], dictionary=id2word, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number of topics  Log likelihood  Perplexity  Coherence\n",
      "0                 5       -7.750877  215.400177   0.514496\n",
      "Best number of topics: 5\n"
     ]
    }
   ],
   "source": [
    "# print Number of topics, Log likelihood, Perplexity and Coherence of lda models as a table\n",
    "df_coherence = pd.DataFrame({'Number of topics': [i+5 for i in range(len(lda_models))],\n",
    "                     'Log likelihood': [lda.log_perplexity(corpus) for lda in lda_models],\n",
    "                        'Perplexity': [np.exp2(-lda.log_perplexity(corpus)) for lda in lda_models],\n",
    "                        'Coherence': coherence_scores})\n",
    "\n",
    "print(df_coherence)\n",
    "\n",
    "# get the best lda model based on coherence score\n",
    "lda = lda_models[np.argmax(coherence_scores)]\n",
    "print(f'Best number of topics: {np.argmax(coherence_scores) + 5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "['US', 'market', 'company', 'sale', 'bank', 'price', 'firm', 'economy', 'growth', 'share']\n",
      "Topic 1:\n",
      "['people', 'game', 'technology', 'phone', 'one', 'user', 'new', 'site', 'could', 'computer']\n",
      "Topic 2:\n",
      "['service', 'broadband', 'people', 'UK', 'BT', 'million', 'music', 'phone', 'digital', 'new']\n",
      "Topic 3:\n",
      "['film', 'US', 'blog', 'new', 'one', 'company', 'director', 'people', 'star', 'life']\n",
      "Topic 4:\n",
      "['government', 'party', 'labour', 'people', 'election', 'minister', 'say', 'blair', 'could', 'new']\n",
      "Topic 5:\n",
      "['game', 'best', 'player', 'one', 'win', 'time', 'world', 'two', 'england', 'play']\n"
     ]
    }
   ],
   "source": [
    "# print topics of lda model\n",
    "for i, topic in lda.show_topics(formatted=False):\n",
    "    print(f'Topic {i}:')\n",
    "    print([i[0] for i in topic])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- The top 10 most and least frequent words were identified after tokenizing the text and removing unwanted tokens. To achieve this, NLTK's English stopword list was employed along with a few custom exceptions.\n",
    "- After lemmatization, the most and least frequent words were determined. This process helped reduce data sparsity and influenced the results, with the least frequent words being impacted the most.\n",
    "- N-grams Analysis, specifically bigrams, proved to be a simple yet effective method for discovering patterns and potential tags in documents. This analysis revealed interesting word combinations such as prime-minister, six-nation, mobile-phone, tony-blair, general-election and new-york.\n",
    "- Part-of-speech (POS) tagging supplied valuable data that can be beneficial for sentiment analysis and language modeling.\n",
    "- Latent Dirichlet Allocation (LDA) was used for topic modeling, with the optimal number of categories determined based on the Coherence metric.\n",
    "- To generate category names based on keywords or tokens for each category, additional steps were taken. OpenAI's GPT-3 model (Davinci) automatically produced these category names using the provided keywords. These category names provided deeper insights into the resulting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bbc.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "model = {\n",
    "    \"stop_words\": stop_words,\n",
    "    \"acronyms\": acronyms,\n",
    "    \"dictionary\": id2word,\n",
    "    \"lda\": lda,\n",
    "    \"categories\": ['Financial/Economic Market', 'Mobile Technology/Gaming', 'Music Industry', 'Film Industry', 'Political Elections/Parties', 'Sports/Gaming']\n",
    "}\n",
    "\n",
    "# save the model to disk\n",
    "joblib.dump(model, 'bbc.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
